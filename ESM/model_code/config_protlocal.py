MAX_SEQUENCES = 5000 # 5k
MAX_RESIDUES = 10000000 #10M

MODEL_CONFIG = {
    "embeddings_dim": 640,
    "init_n_channels": 640,
    "embedding_pretrained": '/root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt',
    "output_dim": 1,
    "dropout": 0.5,
    "kernel_size": 3,
    "conv_dropout": 0.5,
    "max_length": 2500,
    "embedding_args": {
      "arch": "roberta_large",
      "dropout": 0.0,
      "ft_emb_layer_norm_before": True,
      "attention_dropout": 0.0,
      "activation_dropout": 0.0,
      "ffn_embed_dim": 2560,
      "layers": 30,
      "attention_heads": 20,
      "embed_dim": 640,
      "max_positions": 1024,
      "learned_pos": True,
      "activation_fn": "gelu",
      "use_bert_init": True,
      "normalize_before": True,
      "preact_normalize": True,
      "normalize_after": True,
      "token_dropout": True,
      "no_seed_provided": False,
      "pooler_activation_fn": 'tanh',
      "pooler_dropout": 0.0,
      "checkpoint_transformer_block": False,
      "untie_weights_roberta": False,
    },
}

RESIDUE_TRANSLATION = {
    0: "X",
    1: "A",
    2: "C",
    3: "D",
    4: "E",
    5: "F",
    6: "G",
    7: "H",
    8: "I",
    9: "K",
    10: "L",
    11: "M",
    12: "N",
    13: "P",
    14: "Q",
    15: "R",
    16: "S",
    17: "T",
    18: "V",
    19: "W",
    20: "Y",
    21: "U",
}

